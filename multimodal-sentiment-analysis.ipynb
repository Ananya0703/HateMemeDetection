{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n\nWe wanted to develop a model to identify harmful multimodal content. This content combines different modalities, such as text and images, making it difficult for machines to understand.\n\nWhen viewing a meme, for example, we donâ€™t think about the words and photo independently of each other; we understand the combined meaning together. Hence we could not just perform classification on the text or the image alone \n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom tensorflow.keras.utils import load_img\nimport re\nimport string \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Activation, Dropout\nfrom tensorflow.keras.layers import Conv1D, Embedding, GlobalAveragePooling1D \nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.preprocessing import image\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications import VGG16\n\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n\n\nfrom keras.applications import VGG16\nfrom keras import models\nfrom keras import layers\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras import optimizers\nfrom keras import metrics\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-07T09:05:36.182085Z","iopub.execute_input":"2023-04-07T09:05:36.182545Z","iopub.status.idle":"2023-04-07T09:05:39.734091Z","shell.execute_reply.started":"2023-04-07T09:05:36.182510Z","shell.execute_reply":"2023-04-07T09:05:39.732920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Sources\n\n\nWe have used the Memotion Dataset which has 7K annotated memes - with human-annotated tags namely sentiment, and type of humor that is, sarcastic, humorous, or offensive.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/memotion-dataset-7k/memotion_dataset_7k/labels.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:05:39.739678Z","iopub.execute_input":"2023-04-07T09:05:39.740426Z","iopub.status.idle":"2023-04-07T09:05:39.790485Z","shell.execute_reply.started":"2023-04-07T09:05:39.740385Z","shell.execute_reply":"2023-04-07T09:05:39.789362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df.drop(['Unnamed: 0', 'humour', 'sarcasm', 'offensive', 'motivational'],axis = 1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:05:39.792189Z","iopub.execute_input":"2023-04-07T09:05:39.792628Z","iopub.status.idle":"2023-04-07T09:05:39.806780Z","shell.execute_reply.started":"2023-04-07T09:05:39.792590Z","shell.execute_reply":"2023-04-07T09:05:39.805404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.overall_sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:05:39.811015Z","iopub.execute_input":"2023-04-07T09:05:39.811322Z","iopub.status.idle":"2023-04-07T09:05:39.821518Z","shell.execute_reply.started":"2023-04-07T09:05:39.811294Z","shell.execute_reply":"2023-04-07T09:05:39.820499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Negative and Very Negative => 2\n# Positive and Very Positive => 1\n# Neutral => 0\n\ntask_a_labels = {\n    'negative': 2 ,\n    'very_negative': 2,\n    'neutral' : 0,\n    'positive' : 1,\n    'very_positive': 1,\n}\n\ndata['target'] = data['overall_sentiment'].map(task_a_labels)\ndata.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:05:39.823274Z","iopub.execute_input":"2023-04-07T09:05:39.823725Z","iopub.status.idle":"2023-04-07T09:05:39.837365Z","shell.execute_reply.started":"2023-04-07T09:05:39.823670Z","shell.execute_reply":"2023-04-07T09:05:39.836409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop(['text_ocr'], axis=1)\ndata = data.dropna()\ndata","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:05:39.839686Z","iopub.execute_input":"2023-04-07T09:05:39.840314Z","iopub.status.idle":"2023-04-07T09:05:39.859305Z","shell.execute_reply.started":"2023-04-07T09:05:39.840276Z","shell.execute_reply":"2023-04-07T09:05:39.858214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove URLs and hyperlinks\n    text = re.sub(r'https?:\\/\\/[^\\s]+', '', text)\n    \n    # Remove HTML tags and special characters\n    soup = BeautifulSoup(text, 'html.parser')\n    text = soup.get_text()\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    \n    # Tokenize text\n    tokens = word_tokenize(text)\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    \n    # Apply stemming\n    stemmer = PorterStemmer()\n    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n    \n    # Join tokens back into a single string\n    preprocessed_text = ' '.join(stemmed_tokens)\n    \n    return preprocessed_text\n\n# Apply preprocessing to all text in the column\npreprocessed_texts = [preprocess_text(text) for text in data.text_corrected]\n\n# Create Tokenizer\ntokenizer = Tokenizer(num_words=30000)\ntokenizer.fit_on_texts(preprocessed_texts)\n\n# Generate sequences\nsequences = tokenizer.texts_to_sequences(preprocessed_texts)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\n# Pad sequences\npadded_sequences = pad_sequences(sequences, maxlen=100)\nprint('Shape of data tensor:', padded_sequences.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:05:39.860972Z","iopub.execute_input":"2023-04-07T09:05:39.861341Z","iopub.status.idle":"2023-04-07T09:05:44.609881Z","shell.execute_reply.started":"2023-04-07T09:05:39.861300Z","shell.execute_reply":"2023-04-07T09:05:44.608713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['clean_text'] = preprocessed_texts\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:05:44.611629Z","iopub.execute_input":"2023-04-07T09:05:44.612556Z","iopub.status.idle":"2023-04-07T09:05:44.629149Z","shell.execute_reply.started":"2023-04-07T09:05:44.612524Z","shell.execute_reply":"2023-04-07T09:05:44.627906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\nf = open('/kaggle/input/glove6b300dtxt/glove.6B.300d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:05:44.630919Z","iopub.execute_input":"2023-04-07T09:05:44.631293Z","iopub.status.idle":"2023-04-07T09:06:06.857387Z","shell.execute_reply.started":"2023-04-07T09:05:44.631256Z","shell.execute_reply":"2023-04-07T09:06:06.856033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 300\nembedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:06.859080Z","iopub.execute_input":"2023-04-07T09:06:06.859483Z","iopub.status.idle":"2023-04-07T09:06:06.893967Z","shell.execute_reply.started":"2023-04-07T09:06:06.859436Z","shell.execute_reply":"2023-04-07T09:06:06.892972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:06.895536Z","iopub.execute_input":"2023-04-07T09:06:06.895994Z","iopub.status.idle":"2023-04-07T09:06:06.909188Z","shell.execute_reply.started":"2023-04-07T09:06:06.895951Z","shell.execute_reply":"2023-04-07T09:06:06.908007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['image_path'] = data['image_name'].apply(lambda x: '/kaggle/input/memotion-dataset-7k/memotion_dataset_7k/images/'+x)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:06.910567Z","iopub.execute_input":"2023-04-07T09:06:06.911433Z","iopub.status.idle":"2023-04-07T09:06:06.930138Z","shell.execute_reply.started":"2023-04-07T09:06:06.911393Z","shell.execute_reply":"2023-04-07T09:06:06.929029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_names = ['image_121.jpg','image_4802.png','image_6786.jpg','image_6790.jpg','image_6792.jpg']\ndata = data[~data['image_name'].isin(image_names)]\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:06.935222Z","iopub.execute_input":"2023-04-07T09:06:06.935546Z","iopub.status.idle":"2023-04-07T09:06:06.947021Z","shell.execute_reply.started":"2023-04-07T09:06:06.935515Z","shell.execute_reply":"2023-04-07T09:06:06.945883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:06.948378Z","iopub.execute_input":"2023-04-07T09:06:06.949304Z","iopub.status.idle":"2023-04-07T09:06:06.962595Z","shell.execute_reply.started":"2023-04-07T09:06:06.949265Z","shell.execute_reply":"2023-04-07T09:06:06.961543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['target'] = data['target'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:06.963872Z","iopub.execute_input":"2023-04-07T09:06:06.964663Z","iopub.status.idle":"2023-04-07T09:06:06.976363Z","shell.execute_reply.started":"2023-04-07T09:06:06.964623Z","shell.execute_reply":"2023-04-07T09:06:06.974367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:06.979939Z","iopub.execute_input":"2023-04-07T09:06:06.980462Z","iopub.status.idle":"2023-04-07T09:06:06.990938Z","shell.execute_reply.started":"2023-04-07T09:06:06.980434Z","shell.execute_reply":"2023-04-07T09:06:06.988612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport keras\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.utils import load_img, img_to_array\nfrom keras.preprocessing.text import Tokenizer\n\nclass CustomDataGenerator(keras.utils.Sequence):\n    \n    def __init__(self, dataframe, image_size, batch_size, tokenizer, max_text_len):\n        self.df = dataframe\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.max_text_len = max_text_len\n        self.num_classes = len(self.df.target.unique())\n        self.indexes = np.arange(len(self.df))\n        \n    def __len__(self):\n        return int(np.ceil(len(self.df) / self.batch_size))\n    \n    def __getitem__(self, index):\n        batch_indexes = self.indexes[index * self.batch_size:(index+1) * self.batch_size]\n        batch_df = self.df.iloc[batch_indexes]\n        \n        images = []\n        texts = []\n        labels = []\n        \n        for i, row in batch_df.iterrows():\n            # Load image\n            image = load_img(row['image_path'], target_size=self.image_size)\n            image = img_to_array(image)\n            image /= 255.0\n            images.append(image)\n            \n            # Tokenize text\n            text = self.tokenizer.texts_to_sequences([row['clean_text']])[0]\n            text = pad_sequences([text], maxlen=self.max_text_len, padding='post')[0]\n            texts.append(text)\n            \n            # Get label\n            label = row['target']\n            labels.append(label)\n        \n        images = np.array(images)\n        texts = np.array(texts)\n        labels = to_categorical(labels, num_classes=self.num_classes)\n        \n        return [images, texts], labels\n","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:06.992475Z","iopub.execute_input":"2023-04-07T09:06:06.993062Z","iopub.status.idle":"2023-04-07T09:06:07.006291Z","shell.execute_reply.started":"2023-04-07T09:06:06.993024Z","shell.execute_reply":"2023-04-07T09:06:07.005053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:07.009508Z","iopub.execute_input":"2023-04-07T09:06:07.009782Z","iopub.status.idle":"2023-04-07T09:06:07.030929Z","shell.execute_reply.started":"2023-04-07T09:06:07.009757Z","shell.execute_reply":"2023-04-07T09:06:07.029914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define constants\nIMAGE_SIZE = (150, 150)\nBATCH_SIZE = 32\nMAX_TEXT_LEN = 100\n\n# Split data into train and validation sets\ntrain_df = data.sample(frac=0.8, random_state=42)\nval_df = data.drop(train_df.index)\n\n# Create generators\ntrain_generator = CustomDataGenerator(train_df, IMAGE_SIZE, BATCH_SIZE, tokenizer, MAX_TEXT_LEN)\nval_generator = CustomDataGenerator(val_df, IMAGE_SIZE, BATCH_SIZE, tokenizer, MAX_TEXT_LEN)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:07.032427Z","iopub.execute_input":"2023-04-07T09:06:07.033210Z","iopub.status.idle":"2023-04-07T09:06:07.046191Z","shell.execute_reply.started":"2023-04-07T09:06:07.033167Z","shell.execute_reply":"2023-04-07T09:06:07.045290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a dictionary of class labels\nclass_dict = {0: 'neutral', 1: 'positive', 2: 'negative'}\n\nx_batch, y_batch = train_generator.__getitem__(np.random.randint(0, len(train_generator)))\n\n# Display the images and their labels\nplt.figure(figsize=(20,20))\nfor i in range(20):\n    plt.subplot(5,4,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(x_batch[0][i], cmap=plt.cm.binary)\n    plt.xlabel(class_dict[np.argmax(y_batch[i])])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:07.047900Z","iopub.execute_input":"2023-04-07T09:06:07.048639Z","iopub.status.idle":"2023-04-07T09:06:09.041287Z","shell.execute_reply.started":"2023-04-07T09:06:07.048602Z","shell.execute_reply":"2023-04-07T09:06:09.039861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.optimizers import Adam\nfrom keras import backend as K\n\ndef recall(y_true, y_pred):\n\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\n\ndef precision(y_true, y_pred):\n\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\n\ndef f1(y_true, y_pred):\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    return 2 * ((p * r) / (p + r))","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:09.042763Z","iopub.execute_input":"2023-04-07T09:06:09.043286Z","iopub.status.idle":"2023-04-07T09:06:09.053753Z","shell.execute_reply.started":"2023-04-07T09:06:09.043230Z","shell.execute_reply":"2023-04-07T09:06:09.052891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(filepath='best_weights.h5',\n                             monitor='val_f1',\n                             save_best_only=True,\n                             mode='max',\n                             verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:09.055246Z","iopub.execute_input":"2023-04-07T09:06:09.056079Z","iopub.status.idle":"2023-04-07T09:06:09.063064Z","shell.execute_reply.started":"2023-04-07T09:06:09.056038Z","shell.execute_reply":"2023-04-07T09:06:09.062170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################################## - img CNN\ninput_img = Input(shape=(150,150,3))\nmodel = VGG16(weights='imagenet', include_top=False)\nfor layer in model.layers:\n    layer.trainable = False\nx = model(input_img)\nflatten = Flatten()(x)\nflatten = Dense(1024, activation='relu')(flatten)\nflatten = Dense(512, activation='relu')(flatten)\n\n########################################################## - text CNN\n\ninput_txt = Input(shape=(100,), dtype='int32')\nprint(input_txt)\ntxt = layers.Masking(mask_value=0)(input_txt)\ntxt = layers.Embedding(len(word_index) + 1,\n                            300,\n                            weights=[embedding_matrix],\n                            input_length=100,\n                            trainable=False)(txt)\n\ntxt = layers.Conv1D(32, 5)(txt)\ntxt = layers.Conv1D(60, 4)(txt)\ntxt = layers.Conv1D(100, 3)(txt)\ntext_lstm = layers.Bidirectional(layers.LSTM(30,return_sequences=True))(txt)\ntext_lstm = layers.Bidirectional(layers.LSTM(30,return_sequences=True))(text_lstm)\ntext_lstm = layers.Bidirectional(layers.LSTM(30,return_sequences=False))(text_lstm)\ntext_lstm = Dense(512, activation='relu')(text_lstm)\nmerged = keras.layers.concatenate([text_lstm,flatten], axis=1)\n\n################################################################# - final bimodal combination\n\ndense = Dense(1024, activation='relu')(merged)\ndense = Dropout(0.1)(dense) \ndense = Dense(512, activation='relu')(dense)\ndense = Dense(256, activation='relu')(dense)\ndense = Dense(128, activation='relu')(dense)\ndense = Dense(3, activation='softmax')(dense)\nmodel = Model(inputs=(input_img,input_txt), outputs=dense)\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=keras.optimizers.Adam(lr=2e-5),\n              # optimizer=optimizers.RMSprop(),\n              metrics=[\"accuracy\",f1,recall,precision])\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=20,\n    epochs=20,\n    validation_data=val_generator,\n    validation_steps=23)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:06:09.064908Z","iopub.execute_input":"2023-04-07T09:06:09.065685Z","iopub.status.idle":"2023-04-07T09:12:55.839670Z","shell.execute_reply.started":"2023-04-07T09:06:09.065638Z","shell.execute_reply":"2023-04-07T09:12:55.838591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot accuracy curves\nax1.plot(epochs, acc, 'b', label='Training acc')\nax1.plot(epochs, val_acc, 'g', label='Validation acc')\nax1.set_title('Training and validation accuracy')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Accuracy')\nax1.legend()\n\n# Plot loss curves\nax2.plot(epochs, loss, 'b', label='Training loss')\nax2.plot(epochs, val_loss, 'g', label='Validation loss')\nax2.set_title('Training and validation loss')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('Loss')\nax2.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:12:55.841541Z","iopub.execute_input":"2023-04-07T09:12:55.842249Z","iopub.status.idle":"2023-04-07T09:12:56.207892Z","shell.execute_reply.started":"2023-04-07T09:12:55.842203Z","shell.execute_reply":"2023-04-07T09:12:56.206869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get training and validation recall and F1 scores\ntrain_recall = history.history['recall']\nval_recall = history.history['val_recall']\ntrain_f1 = history.history['f1']\nval_f1 = history.history['val_f1']\n\nepochs = range(1, len(train_recall) + 1)\n\n# Plot recall and F1 score side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n\n# Plot recall\nax1.plot(epochs, train_recall, 'b', label='Training Recall')\nax1.plot(epochs, val_recall, 'r', label='Validation Recall')\nax1.set_title('Training and Validation Recall')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Recall')\nax1.legend()\n\n# Plot F1 score\nax2.plot(epochs, train_f1, 'b', label='Training F1 Score')\nax2.plot(epochs, val_f1, 'r', label='Validation F1 Score')\nax2.set_title('Training and Validation F1 Score')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('F1 Score')\nax2.legend()\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:12:56.209585Z","iopub.execute_input":"2023-04-07T09:12:56.210309Z","iopub.status.idle":"2023-04-07T09:12:56.580724Z","shell.execute_reply.started":"2023-04-07T09:12:56.210269Z","shell.execute_reply":"2023-04-07T09:12:56.579772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_samples = len(val_generator) * BATCH_SIZE\nsteps = total_samples // BATCH_SIZE\nprint(steps)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:12:56.582391Z","iopub.execute_input":"2023-04-07T09:12:56.583104Z","iopub.status.idle":"2023-04-07T09:12:56.589378Z","shell.execute_reply.started":"2023-04-07T09:12:56.583063Z","shell.execute_reply":"2023-04-07T09:12:56.587953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('best_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:12:56.591218Z","iopub.execute_input":"2023-04-07T09:12:56.591986Z","iopub.status.idle":"2023-04-07T09:12:56.812034Z","shell.execute_reply.started":"2023-04-07T09:12:56.591948Z","shell.execute_reply":"2023-04-07T09:12:56.810907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = model.evaluate_generator(val_generator,steps=44)\nprint(a)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:12:56.813804Z","iopub.execute_input":"2023-04-07T09:12:56.814271Z","iopub.status.idle":"2023-04-07T09:13:09.398795Z","shell.execute_reply.started":"2023-04-07T09:12:56.814224Z","shell.execute_reply":"2023-04-07T09:13:09.397546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in zip(model.metrics_names,a):\n  print(x)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:13:09.400504Z","iopub.execute_input":"2023-04-07T09:13:09.400925Z","iopub.status.idle":"2023-04-07T09:13:09.407321Z","shell.execute_reply.started":"2023-04-07T09:13:09.400882Z","shell.execute_reply":"2023-04-07T09:13:09.406137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predicted labels\ny_pred = model.predict(val_generator)\ny_pred_labels = np.argmax(y_pred, axis=1)\n\n# Get true labels\ny_true_labels = val_df['target'].values\n\n# Get image paths and text for display\nimage_paths = val_df['image_path'].values\ntexts = val_df['clean_text'].values\n\nclass_dict = {0: 'neutral', 1: 'positive', 2: 'negative'}\n\n\n# Display some examples\nfor i in range(10):\n    # Load image\n    image = load_img(image_paths[i], target_size=IMAGE_SIZE)\n    \n    # Get predicted and true labels\n    pred_label = y_pred_labels[i]\n    true_label = y_true_labels[i]\n    \n    # Get text\n    text = texts[i]\n    \n    # Print predicted and true labels, and text\n    print('Image:', image_paths[i])\n    print('Predicted label:', pred_label)\n    print('True label:', true_label)\n    print('Text:', text)\n    print('')\n    \n    # Show image\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-07T09:13:09.409043Z","iopub.execute_input":"2023-04-07T09:13:09.409886Z","iopub.status.idle":"2023-04-07T09:13:24.945518Z","shell.execute_reply.started":"2023-04-07T09:13:09.409827Z","shell.execute_reply":"2023-04-07T09:13:24.943900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}